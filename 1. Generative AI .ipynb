{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b30785",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\" width=150px> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>Generative AI with OpenAI API</h1>\n",
    "<h1>Basic Concepts</h1>\n",
    "        <p>Bruno Gonçalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb51a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "import tqdm as tq\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3d40f",
   "metadata": {},
   "source": [
    "We start by printing out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3961dbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.9\n",
      "IPython version      : 8.10.0\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 22.5.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: ba6ac58b56bc68293ed7b7a52d0b11256ce5d56d\n",
      "\n",
      "tqdm      : 4.64.1\n",
      "openai    : 0.28.1\n",
      "json      : 2.0.9\n",
      "nltk      : 3.7\n",
      "numpy     : 1.23.5\n",
      "matplotlib: 3.7.2\n",
      "watermark : 2.4.2\n",
      "pandas    : 1.5.3\n",
      "tiktoken  : 0.5.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2abfec",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a397f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('d4sci.mplstyle')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8604d6ea",
   "metadata": {},
   "source": [
    "# Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7bb85",
   "metadata": {},
   "source": [
    "tiktoken supports several types of encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba900cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7046b7",
   "metadata": {},
   "source": [
    "Encodings can by loaded by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07980bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603dd31",
   "metadata": {},
   "source": [
    "or by specifying the name of the model we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21285071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1c630",
   "metadata": {},
   "source": [
    "After loading, we can use the encoding object to tokenize text by using the __encode()__ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf15950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 1609, 5963, 374, 2294, 0]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = encoding.encode(\"tiktoken is great!\")\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd044f9",
   "metadata": {},
   "source": [
    "which returns numerical IDs for each of the tokens. Numerical IDs can be converted back to the original text using __decode()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c913bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7729b",
   "metadata": {},
   "source": [
    "Tokens can be individual letters, characters, or even full words. To convert indivudal numerical IDs to tokens, we should use __decode_single_token_bytes()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689f5140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\t->\tb't'\n",
      "1609\t->\tb'ik'\n",
      "5963\t->\tb'token'\n",
      "374\t->\tb' is'\n",
      "2294\t->\tb' great'\n",
      "0\t->\tb'!'\n"
     ]
    }
   ],
   "source": [
    "for token in encoded_text:\n",
    "    print('%s\\t->\\t%s' % (token, encoding.decode_single_token_bytes(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16adcd19",
   "metadata": {},
   "source": [
    "The number of tokens generated depend on the specific encoding used. This is particularly noticeable in long words, so let's take the longest english word as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a75f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gpt2: 15 tokens\n",
      "Token IDs: [79, 25668, 261, 25955, 859, 2500, 1416, 404, 873, 41896, 709, 349, 5171, 36221, 42960]\n",
      "Token Text: [b'p', b'neum', b'on', b'oult', b'ram', b'icro', b'sc', b'op', b'ics', b'ilic', b'ov', b'ol', b'can', b'ocon', b'iosis']\n",
      "\n",
      "r50k_base: 15 tokens\n",
      "Token IDs: [79, 25668, 261, 25955, 859, 2500, 1416, 404, 873, 41896, 709, 349, 5171, 36221, 42960]\n",
      "Token Text: [b'p', b'neum', b'on', b'oult', b'ram', b'icro', b'sc', b'op', b'ics', b'ilic', b'ov', b'ol', b'can', b'ocon', b'iosis']\n",
      "\n",
      "p50k_base: 15 tokens\n",
      "Token IDs: [79, 25668, 261, 25955, 859, 2500, 1416, 404, 873, 41896, 709, 349, 5171, 36221, 42960]\n",
      "Token Text: [b'p', b'neum', b'on', b'oult', b'ram', b'icro', b'sc', b'op', b'ics', b'ilic', b'ov', b'ol', b'can', b'ocon', b'iosis']\n",
      "\n",
      "p50k_edit: 15 tokens\n",
      "Token IDs: [79, 25668, 261, 25955, 859, 2500, 1416, 404, 873, 41896, 709, 349, 5171, 36221, 42960]\n",
      "Token Text: [b'p', b'neum', b'on', b'oult', b'ram', b'icro', b'sc', b'op', b'ics', b'ilic', b'ov', b'ol', b'can', b'ocon', b'iosis']\n",
      "\n",
      "cl100k_base: 17 tokens\n",
      "Token IDs: [79, 818, 372, 263, 11206, 99040, 2823, 2445, 454, 1233, 321, 292, 869, 337, 69377, 444, 91260]\n",
      "Token Text: [b'p', b'ne', b'um', b'on', b'oul', b'tram', b'icro', b'sc', b'op', b'ics', b'il', b'ic', b'ov', b'ol', b'cano', b'con', b'iosis']\n"
     ]
    }
   ],
   "source": [
    "example_string = \"pneumonoultramicroscopicsilicovolcanoconiosis\"\n",
    "\n",
    "for encoding_name in tiktoken.list_encoding_names():\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    encoded_text = encoding.encode(example_string)\n",
    "    num_tokens = len(encoded_text)\n",
    "    token_text = [encoding.decode_single_token_bytes(token) for token in encoded_text]\n",
    "    print()\n",
    "    print(f\"{encoding_name}: {num_tokens} tokens\")\n",
    "    print(f\"Token IDs: {encoded_text}\")\n",
    "    print(f\"Token Text: {token_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827c316",
   "metadata": {},
   "source": [
    "Encodings are capable of handling a large number of languages and character sets. Let's take Japanese for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de7eadb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gpt2: 14 tokens\n",
      "Token IDs: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "Token Text: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "r50k_base: 14 tokens\n",
      "Token IDs: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "Token Text: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "p50k_base: 14 tokens\n",
      "Token IDs: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "Token Text: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "p50k_edit: 14 tokens\n",
      "Token IDs: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "Token Text: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "cl100k_base: 9 tokens\n",
      "Token IDs: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
      "Token Text: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
     ]
    }
   ],
   "source": [
    "example_string = \"お誕生日おめでとう\"\n",
    "\n",
    "for encoding_name in tiktoken.list_encoding_names():\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    encoded_text = encoding.encode(example_string)\n",
    "    num_tokens = len(encoded_text)\n",
    "    token_text = [encoding.decode_single_token_bytes(token) for token in encoded_text]\n",
    "    print()\n",
    "    print(f\"{encoding_name}: {num_tokens} tokens\")\n",
    "    print(f\"Token IDs: {encoded_text}\")\n",
    "    print(f\"Token Text: {token_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da17ecf",
   "metadata": {},
   "source": [
    "Here we are seeing the unicode representation of the text, but we can easily recover the original Kanji and Hiragana text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcef74a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "お誕生日おめでとう\n"
     ]
    }
   ],
   "source": [
    "print(b\"\".join(token_text).decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abfa353",
   "metadata": {},
   "source": [
    "# Counting tokens for API calls\n",
    "\n",
    "We can use tiktoken to count how many tokens our API calls are going to consume. Naturally, this depends on the language model used. Based on https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77fa65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            n_tokens = len(encoding.encode(value))\n",
    "            num_tokens += n_tokens\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2875eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      "127 prompt tokens counted by num_tokens_from_messages().\n",
      "127 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-3.5-turbo-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-3.5-turbo\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-0314\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4\",\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=example_messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1,  # we're only counting input tokens here, so let's not waste tokens on the output\n",
    "    )\n",
    "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da0506",
   "metadata": {},
   "source": [
    "# \"Small\" Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b28d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bccdd6",
   "metadata": {},
   "source": [
    "We start by counting number of trigram co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bc64130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e3d56d03d241f7a450f405f660f91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sentence in tqdm(reuters.sents(), total=54_711):\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        bigram = (w1, w2)\n",
    "        model[bigram][w3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca57dc",
   "metadata": {},
   "source": [
    "And normalizing the probabilities for each bigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffbda658",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in model:\n",
    "    total_count = float(sum(model[bigram].values()))\n",
    "\n",
    "    for w3 in model[bigram]:\n",
    "        model[bigram][w3] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da439c",
   "metadata": {},
   "source": [
    "Our language model is just a weighted mapping between each bigram and the possible next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c3ea9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "            {'States': 0.880672268907563,\n",
       "             'Kingdom': 0.011764705882352941,\n",
       "             'Arab': 0.052100840336134456,\n",
       "             'Permanent': 0.0016806722689075631,\n",
       "             'Steelworkers': 0.0033613445378151263,\n",
       "             'Nations': 0.025210084033613446,\n",
       "             'Coconut': 0.0067226890756302525,\n",
       "             'State': 0.0033613445378151263,\n",
       "             'Democratic': 0.0016806722689075631,\n",
       "             'Food': 0.008403361344537815,\n",
       "             'Automobile': 0.0016806722689075631,\n",
       "             'acquisition': 0.0016806722689075631,\n",
       "             'Rubber': 0.0016806722689075631})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[(\"the\", \"United\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "571d60a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "            {',': 0.21428571428571427,\n",
       "             'and': 0.21428571428571427,\n",
       "             'blender': 0.07142857142857142,\n",
       "             ')': 0.14285714285714285,\n",
       "             'company': 0.07142857142857142,\n",
       "             'operations': 0.07142857142857142,\n",
       "             'assets': 0.07142857142857142,\n",
       "             'Ltd': 0.07142857142857142,\n",
       "             '.': 0.07142857142857142})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[(\"United\", \"Kingdom\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3a8cb",
   "metadata": {},
   "source": [
    "This is all we need to generate new text staring from a bigram prompt. We must simply perform a random walk on this weighted graph starting from an initial prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d01a3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_from_prompt(prompt):\n",
    "    text = [*prompt]\n",
    "\n",
    "    # Dont impose any fixed sentence length\n",
    "    while True:\n",
    "        # the current not we're in is just the one that accounts\n",
    "        # for the last two words in the text\n",
    "        bigram = tuple(text[-2:])\n",
    "\n",
    "        # We extract the list of possible next words and their probabilities\n",
    "        words = []\n",
    "        probs = []\n",
    "\n",
    "        for word, prob in model[bigram].items():\n",
    "            words.append(word)\n",
    "            probs.append(prob)\n",
    "\n",
    "        # Choose one word proportionally to each probability\n",
    "        selection = np.random.multinomial(1, probs)\n",
    "        pos = np.argmax(selection)\n",
    "        word = words[pos]\n",
    "\n",
    "        # Append the new word to our runnning text\n",
    "        text.append(word)\n",
    "\n",
    "        # Wtop when we hit two None tokens in a row, that represnet the end of a sentence\n",
    "        if text[-2:] == [None, None]:\n",
    "            break\n",
    "\n",
    "    return \" \".join([t for t in text if t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6dd3ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United States and the purchase includes seven cts Net 2 , 025 , 961'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence_from_prompt(('United', 'States'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "288ed7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'today the options can be put on positions .'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence_from_prompt(('today', 'the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6127477f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'financial markets about the future ,\" Weinberger said on Friday after an earthquake last March , against last week .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence_from_prompt(('financial', 'markets'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce192514",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
